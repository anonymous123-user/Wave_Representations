{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch\n",
    "from utils import make_model, set_random_seed, save_model, load_model\n",
    "from trainer import train\n",
    "from dataset import ShapeDataset, load_data\n",
    "from dataset_config import DATASET_CONFIG\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import fastcluster\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from plotting import plot_phases, plot_results, plot_eval, plot_fourier, plot_phases2, plot_masks, plot_slots, build_color_mask, plot_clusters, plot_clusters2\n",
    "\n",
    "from loss_metrics import get_ar_metrics, compute_pixelwise_accuracy, compute_iou\n",
    "from loss_metrics import compute_pixelwise_accuracy\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a YAML file\n",
    "def load_yaml_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"mnist\", \"new_tetronimoes\"]\n",
    "model_types = [\"baseline3\"]\n",
    "seed_folders = [\"1\", \"2\", \"3\"]\n",
    "\n",
    "extensions = []\n",
    "for curr1 in datasets:\n",
    "    for curr2 in model_types:\n",
    "        for curr3 in seed_folders:\n",
    "            extensions.append(f\"{curr1}/{curr2}/{curr3}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mnist/baseline3/1/',\n",
       " 'mnist/baseline3/2/',\n",
       " 'mnist/baseline3/3/',\n",
       " 'new_tetronimoes/baseline3/1/',\n",
       " 'new_tetronimoes/baseline3/2/',\n",
       " 'new_tetronimoes/baseline3/3/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder = '/n/ba_lab/Everyone/mjacobs/projects/shape-object_store/realign3/'\n",
    "folder_new = 'experiments/'\n",
    "hydra_config_file = '.hydra/config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths = []\n",
    "folders = []\n",
    "for ext in extensions:\n",
    "    #if 'mnist' in ext or 'hidden' in ext: # re-ran mnist results, also ran hidden results, stored in realign4\n",
    "    #    ext = folder_new + ext\n",
    "    #else:\n",
    "    #    ext = folder + ext\n",
    "    ext = folder_new + ext\n",
    "    # Use glob to find the specific file in each directory\n",
    "    search_pattern = os.path.join(ext, '**', '.hydra', 'config.yaml')\n",
    "    for file_path in glob.iglob(search_pattern, recursive=True):\n",
    "        # Check if it's a file\n",
    "        if os.path.isfile(file_path):\n",
    "            fpaths.append(file_path)\n",
    "            folders.append(os.path.dirname(os.path.dirname(file_path)) + \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/n/ba_lab/Everyone/mjacobs/projects/shape-object_store/realign4/mnist/baseline3/1/lstm_100/.hydra/config.yaml',\n",
       " '/n/ba_lab/Everyone/mjacobs/projects/shape-object_store/realign4/mnist/baseline3/1/rnn_100/.hydra/config.yaml',\n",
       " '/n/ba_lab/Everyone/mjacobs/projects/shape-object_store/realign4/mnist/baseline3/2/lstm_100/.hydra/config.yaml',\n",
       " '/n/ba_lab/Everyone/mjacobs/projects/shape-object_store/realign4/mnist/baseline3/2/rnn_100/.hydra/config.yaml',\n",
       " '/n/ba_lab/Everyone/mjacobs/projects/shape-object_store/realign4/mnist/baseline3/3/lstm_100/.hydra/config.yaml',\n",
       " '/n/ba_lab/Everyone/mjacobs/projects/shape-object_store/realign4/mnist/baseline3/3/rnn_100/.hydra/config.yaml',\n",
       " '/n/ba_lab/Everyone/mjacobs/projects/shape-object_store/realign4/new_tetronimoes/baseline3/1/lstm_100/.hydra/config.yaml',\n",
       " '/n/ba_lab/Everyone/mjacobs/projects/shape-object_store/realign4/new_tetronimoes/baseline3/1/rnn_100/.hydra/config.yaml',\n",
       " '/n/ba_lab/Everyone/mjacobs/projects/shape-object_store/realign4/new_tetronimoes/baseline3/2/lstm_100/.hydra/config.yaml',\n",
       " '/n/ba_lab/Everyone/mjacobs/projects/shape-object_store/realign4/new_tetronimoes/baseline3/2/rnn_100/.hydra/config.yaml',\n",
       " '/n/ba_lab/Everyone/mjacobs/projects/shape-object_store/realign4/new_tetronimoes/baseline3/3/lstm_100/.hydra/config.yaml',\n",
       " '/n/ba_lab/Everyone/mjacobs/projects/shape-object_store/realign4/new_tetronimoes/baseline3/3/rnn_100/.hydra/config.yaml']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [load_yaml_file(p) for p in fpaths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import MNISTSegmentationDataset, load_new_tetrominoes, ShapeDataset\n",
    "from torchvision import datasets as torchvision_datasets\n",
    "\n",
    "def load_data(dataset, data_config, num_train=None, num_test=None, scale_min=0.7, transform_set='set1', normalize=True):\n",
    "    if dataset == 'mnist':\n",
    "        testset = torchvision_datasets.MNIST(data_config['test_path'], train=False, download=False,\n",
    "                                             transform=transforms.ToTensor())\n",
    "        testset = MNISTSegmentationDataset(testset, data_config['img_size'])\n",
    "        return testset\n",
    "    elif dataset == 'new_tetronimoes':\n",
    "        x, y = load_new_tetrominoes(data_config['x_test_path'], data_config['y_test_path'])\n",
    "        return ShapeDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Setup\n",
    "seed = 7\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_random_seed(seed)\n",
    "\n",
    "# Load data\n",
    "data_config1 = DATASET_CONFIG['new_tetronimoes']\n",
    "data_config2 = DATASET_CONFIG['mnist']\n",
    "testset1 = load_data('new_tetronimoes', data_config1)\n",
    "testset2 = load_data('mnist', data_config2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved models\n",
    "def load_model(cp_folder, config, device, data_config):\n",
    "    data_config = data_config[config['dataset']]\n",
    "    if 'cell_type' in config.keys():\n",
    "        cell_type = config['cell_type']\n",
    "    else:\n",
    "        cell_type = None\n",
    "    if 'dt1' in config.keys():\n",
    "        dt = config['dt1']\n",
    "    else:\n",
    "        dt = config['dt']\n",
    "    # Make model\n",
    "    net = make_model(\n",
    "        device,\n",
    "        config['block_type'],\n",
    "        config['model_type'],\n",
    "        config['oscillator_type'],\n",
    "        config['num_classes'],\n",
    "        config['N'],\n",
    "        config['M'],\n",
    "        dt,\n",
    "        config['min_iters'],\n",
    "        config['max_iters'],\n",
    "        data_config['channels'],\n",
    "        config['hidden_channels'],\n",
    "        config['rnn_kernel'],\n",
    "        config['num_blocks'],\n",
    "        config['num_slots'],\n",
    "        config['num_iters'],\n",
    "        data_config['img_size'],\n",
    "        config['kernel_init'],\n",
    "        cell_type=cell_type,\n",
    "        num_layers=config['num_layers'],\n",
    "    )\n",
    "\n",
    "    net.load_state_dict(torch.load(cp_folder + \"cp.pt\"), strict=False)\n",
    "    net.eval()\n",
    "    return net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [load_model(folders[i], configs[i], device, DATASET_CONFIG) for i in range(len(folders))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More score functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acc(predictions, ground_truth, ignore_class=0):\n",
    "    \"\"\"\n",
    "    Calculate the average pixel-wise accuracy for each image in the batch.\n",
    "\n",
    "    Args:\n",
    "        predictions (torch.Tensor): Predicted segmentation masks, shape (B, H, W)\n",
    "        ground_truth (torch.Tensor): Ground truth segmentation masks, shape (B, H, W)\n",
    "        ignore_class (int, optional): Class to ignore in accuracy calculation. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Average pixel-wise accuracy for each image, shape (B,)\n",
    "    \"\"\"\n",
    "    # Ensure the tensors are of the same shape\n",
    "    if predictions.shape != ground_truth.shape:\n",
    "        raise ValueError(\"Shape of predictions and ground_truth must match.\")\n",
    "\n",
    "    # Create a mask for pixels that are NOT the ignore_class\n",
    "    mask = ground_truth != ignore_class  # Shape: (B, H, W)\n",
    "\n",
    "    # Calculate correct predictions where mask is True\n",
    "    correct = (predictions == ground_truth) & mask  # Shape: (B, H, W)\n",
    "\n",
    "    # Sum correct predictions per image\n",
    "    correct_per_image = correct.view(correct.size(0), -1).sum(dim=1).float()  # Shape: (B,)\n",
    "\n",
    "    # Sum valid (non-ignored) pixels per image\n",
    "    total_per_image = mask.view(mask.size(0), -1).sum(dim=1).float()  # Shape: (B,)\n",
    "\n",
    "    # Calculate accuracy per image\n",
    "    accuracy = correct_per_image / total_per_image  # Shape: (B,)\n",
    "    accuracy = accuracy.cpu().numpy()\n",
    "    return np.sum(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou(predictions, ground_truth, ignore_class=0, num_classes=None):\n",
    "    \"\"\"\n",
    "    Calculate the mean Intersection over Union (IoU) for each image in the batch.\n",
    "\n",
    "    Args:\n",
    "        predictions (torch.Tensor): Predicted segmentation masks, shape (B, H, W)\n",
    "        ground_truth (torch.Tensor): Ground truth segmentation masks, shape (B, H, W)\n",
    "        ignore_class (int, optional): Class to ignore in IoU calculation. Default is 0.\n",
    "        num_classes (int, optional): Total number of classes. If None, inferred from data.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Mean IoU for each image, shape (B,)\n",
    "    \"\"\"\n",
    "    if predictions.shape != ground_truth.shape:\n",
    "        raise ValueError(\"Shape of predictions and ground_truth must match.\")\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = int(max(predictions.max(), ground_truth.max()) + 1)\n",
    "\n",
    "    batch_size = predictions.size(0)\n",
    "\n",
    "    # Create a mask to ignore the specified class\n",
    "    mask = ground_truth != ignore_class  # Shape: (B, H, W)\n",
    "    mask = mask.unsqueeze(1)\n",
    "\n",
    "    # Expand dimensions to (B, C, H, W) for one-hot encoding\n",
    "    predictions_one_hot = torch.nn.functional.one_hot(predictions, num_classes=num_classes).permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "    ground_truth_one_hot = torch.nn.functional.one_hot(ground_truth, num_classes=num_classes).permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "\n",
    "    # APPLY MASK\n",
    "    predictions_one_hot = predictions_one_hot * mask\n",
    "    ground_truth_one_hot = ground_truth_one_hot * mask\n",
    "    intersection = (predictions_one_hot & ground_truth_one_hot) # B x C x H x W\n",
    "    union = (predictions_one_hot | ground_truth_one_hot) # B x C x H x W\n",
    "\n",
    "    intersection = intersection.sum((1, 2, 3))\n",
    "    union = union.sum((1, 2, 3))\n",
    "    iou = intersection / union\n",
    "    iou = iou.cpu().numpy()\n",
    "    return np.sum(iou)\n",
    "    \n",
    "\n",
    "    ious = []\n",
    "    for i in range(batch_size):\n",
    "        # Exclude the ignore_class\n",
    "        curr_preds = predictions_one_hot[i][mask[i]]\n",
    "        curr_gt = ground_truth_one_hot[i][mask[i]]\n",
    "        \n",
    "        # Compute intersection and union\n",
    "        intersection = (curr_preds & curr_gt).float().sum()\n",
    "        union = (curr_preds | curr_gt).float().sum()\n",
    "\n",
    "        # Avoid division by zero\n",
    "        #eps = 1e-6\n",
    "        #iou = intersection / (union + eps)\n",
    "        iou = intersection / union\n",
    "        ious.append(iou.item())\n",
    "\n",
    "    iou = iou.cpu().numpy()\n",
    "    return np.sum(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ari(predictions, ground_truth, ignore_class=0):\n",
    "    \"\"\"\n",
    "    Calculate the Adjusted Rand Index (ARI) for each image in the batch.\n",
    "\n",
    "    Args:\n",
    "        predictions (torch.Tensor): Predicted segmentation masks, shape (B, H, W)\n",
    "        ground_truth (torch.Tensor): Ground truth segmentation masks, shape (B, H, W)\n",
    "        ignore_class (int, optional): Class to ignore in ARI calculation. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Adjusted Rand Index for each image, shape (B,)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "    B = predictions.shape[0]\n",
    "    ari_scores = []\n",
    "\n",
    "    # Iterate over the batch\n",
    "    for i in range(B):\n",
    "        pred = predictions[i].flatten()\n",
    "        gt = ground_truth[i].flatten()\n",
    "\n",
    "        # Create mask to ignore the specified class\n",
    "        mask = gt != ignore_class\n",
    "        pred_masked = pred[mask]\n",
    "        gt_masked = gt[mask]\n",
    "\n",
    "        # Convert tensors to numpy arrays for sklearn\n",
    "        pred_np = pred_masked.cpu().numpy()\n",
    "        gt_np = gt_masked.cpu().numpy()\n",
    "\n",
    "        # Calculate ARI using sklearn\n",
    "        ari = adjusted_rand_score(gt_np, pred_np)\n",
    "        ari_scores.append(ari)\n",
    "\n",
    "    return np.sum(ari_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_scores(net, testset, device, batch_size, num_classes, ignore_class=0):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_iou = 0\n",
    "    total_ari = 0\n",
    "\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    with torch.no_grad():\n",
    "        for x in testloader:\n",
    "            x, x_target = x\n",
    "            x_target = x_target.to(device).type(torch.long)\n",
    "            x = x.to(device)\n",
    "            b, c, h, w = x.size()\n",
    "            x_pred_classifier = net(x)\n",
    "            # LOSS\n",
    "            loss = loss_func(x_pred_classifier, x_target)\n",
    "            loss = loss.item()\n",
    "            # CLASSIFIER ARI\n",
    "            x_pred_classifier = torch.argmax(x_pred_classifier, dim=1)\n",
    "            \n",
    "            acc = calculate_acc(x_pred_classifier, x_target, ignore_class=ignore_class)\n",
    "            iou = calc_iou(x_pred_classifier, x_target, num_classes=num_classes, ignore_class=ignore_class)\n",
    "            #ari = calc_ari(x_pred_classifier, x_target, ignore_class=ignore_class)\n",
    "\n",
    "            # Store\n",
    "            total_loss += loss * b\n",
    "            total_acc += acc\n",
    "            total_iou += iou\n",
    "            #total_ari += ari\n",
    "\n",
    "    num_samples = len(testset)\n",
    "    loss = total_loss / num_samples\n",
    "    #ari = total_ari / num_samples\n",
    "    iou = total_iou / num_samples\n",
    "    acc = total_acc / num_samples\n",
    "\n",
    "    return loss, iou, acc\n",
    "    #return loss, ari, iou, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsets = {\n",
    "    'new_tetronimoes' : testset1,\n",
    "    'mnist' : testset2,\n",
    "}\n",
    "ignore_class = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/12\n",
      "2/12\n",
      "3/12\n",
      "4/12\n",
      "5/12\n",
      "6/12\n",
      "7/12\n",
      "8/12\n",
      "9/12\n",
      "10/12\n",
      "11/12\n",
      "12/12\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i, net in enumerate(models):\n",
    "    print(f\"{i + 1}/{len(models)}\")\n",
    "    config = configs[i]\n",
    "    score = eval_scores(net, testsets[config['dataset']], device, batch_size=64, num_classes=config['num_classes'], ignore_class=ignore_class)\n",
    "    scores.append(score)\n",
    "    if i == 24:\n",
    "        np.save('scores_halfway_forgot.npy', np.array(scores))\n",
    "scores = np.array(scores)\n",
    "np.save(\"scores_forgot.npy\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = {}\n",
    "for i, net in enumerate(models):\n",
    "    config = configs[i]\n",
    "    score = scores[i]\n",
    "    dataset = config['dataset']\n",
    "    model_type = config['model_type']\n",
    "    if dataset not in all_scores:\n",
    "        all_scores[dataset] = {}\n",
    "\n",
    "    # handle special case where we have lstm vs rnn\n",
    "    full_model_type = model_type\n",
    "    if model_type == 'baseline3_fft' or model_type == 'baseline3':\n",
    "        cell_type = config['cell_type']\n",
    "        full_model_type = f\"{model_type}-{cell_type}\"\n",
    "    if model_type == 'baseline1_flexible':\n",
    "        num_layers = config['num_layers']\n",
    "        full_model_type = f\"{model_type}-{num_layers}\"\n",
    "    if full_model_type not in all_scores[dataset]:\n",
    "        all_scores[dataset][full_model_type] = {\n",
    "            'loss' : [],\n",
    "            #'ari' : [],\n",
    "            'iou' : [],\n",
    "            'acc' : []\n",
    "        }\n",
    "    all_scores[dataset][full_model_type]['loss'].append(score[0])\n",
    "    #all_scores[dataset][full_model_type]['ari'].append(score[1])\n",
    "    all_scores[dataset][full_model_type]['iou'].append(score[1])\n",
    "    all_scores[dataset][full_model_type]['acc'].append(score[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_stats = {}\n",
    "for dataset in all_scores:\n",
    "    score_stats[dataset] = {}\n",
    "    for model_type in all_scores[dataset]:\n",
    "        score_stats[dataset][model_type] = {}\n",
    "        curr_scores = all_scores[dataset][model_type]\n",
    "        for metric in curr_scores:\n",
    "            mean = np.mean(curr_scores[metric])\n",
    "            std = np.std(curr_scores[metric])\n",
    "            score_stats[dataset][model_type][metric] = mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mnist': {'baseline3-lstm': {'loss': (0.6948826508522034,\n",
       "    7.92936117696532e-05),\n",
       "   'iou': (0.0, 0.0),\n",
       "   'acc': (0.0, 0.0)},\n",
       "  'baseline3-rnn': {'loss': (0.6949432949384055, 3.315571287538999e-05),\n",
       "   'iou': (0.0, 0.0),\n",
       "   'acc': (0.0, 0.0)}},\n",
       " 'new_tetronimoes': {'baseline3-lstm': {'loss': (0.6459340534210205,\n",
       "    0.00031856562923701377),\n",
       "   'iou': (0.0, 0.0),\n",
       "   'acc': (0.0, 0.0)},\n",
       "  'baseline3-rnn': {'loss': (0.6455641514460245, 0.00013795588264729538),\n",
       "   'iou': (0.0, 0.0),\n",
       "   'acc': (0.0, 0.0)}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"score_stats_forgot.json\", 'w') as json_file:\n",
    "    json.dump(score_stats, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slot_attention6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
