wandb:
  project: object-shape
  entity:
hydra:
  run:
    dir: /${now:%Y-%m-%d}/${now:%H-%M-%S} #${now:%Y-%m-%d}/${now:%H-%M-%S}
params:
  run_name: "${now:%Y-%m-%d}/${now:%H-%M-%S}/new_tetronimoes/b1f_16l"
  model_type: "baseline3" # model1=cornn, model2=slot attention, model3=baseline conv
  block_type: "block18"
  oscillator_type: "cornn" #"d-kuramoto" # cornn, kuramoto, d-kuramoto, b-kuramoto, cornn2, cornn3
  dataset: "mnist" # options: 'two-shapes' 'tetrominoes' '2-4Shapes'
  N: 56 # use 32 for two-shapes, 35 for tetrominoes, 40 for 2-4Shapes (unless you change encoder feature resolution)
  scale_min: 0.5 # torch default is 0.08
  num_train: 10000
  num_test: 100
  batch_size: 16
  min_epochs: 25
  max_epochs: 50
  lr: 1.0e-3
  seed: 6
  M: 8
  min_iters: 0
  max_iters: 50
  dt: 0.1
  num_classes: 11
  hidden_channels: 16
  rnn_kernel: 3
  num_blocks: 1
  num_slots: 256
  num_iters: 4
  kernel_init: 'op' # options: 'vanilla' 'op' 'pk'
  cp_path: "cp.pt"
  save_model: True
  num_channels_plot: 16 # to plot all, set num_channels_plot to null or the value of hidden_channels
  temp: 0.1
  normalize: True
  transform_set: "set2"
  optimizer: 'adam' # adam, adamw
  weight_decay: 0.01
  cell_type: 'lstm' # rnn, gru, lstm
  num_layers: 16 # for baseline
  training_patience: 10
  training_tolerance: 0.001